{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1:0\n",
      "[ 1.]\n",
      "a_name_scope/var2:0\n",
      "[ 2.]\n",
      "a_name_scope/var2_1:0\n",
      "[ 2.0999999]\n",
      "a_name_scope/var2_2:0\n",
      "[ 2.20000005]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope(\"a_name_scope\"):\n",
    "    initializer = tf.constant_initializer(value=1)\n",
    "    var1 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)\n",
    "    var2 = tf.Variable(name='var2', initial_value=[2], dtype=tf.float32)\n",
    "    var21 = tf.Variable(name='var2', initial_value=[2.1], dtype=tf.float32)\n",
    "    var22 = tf.Variable(name='var2', initial_value=[2.2], dtype=tf.float32)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(var1.name)        # var1:0\n",
    "    print(sess.run(var1))   # [ 1.]\n",
    "    print(var2.name)        # a_name_scope/var2:0\n",
    "    print(sess.run(var2))   # [ 2.]\n",
    "    print(var21.name)       # a_name_scope/var2_1:0\n",
    "    print(sess.run(var21))  # [ 2.0999999]\n",
    "    print(var22.name)       # a_name_scope/var2_2:0\n",
    "    print(sess.run(var22))  # [ 2.20000005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出使用 tf.Variable() 定义的时候, 虽然 name 都一样, 但是为了不重复变量名, Tensorflow 输出的变量名并不是一样的. 所以, 本质上 var2, var21, var22 并不是一样的变量. 而另一方面, 使用tf.get_variable()定义的变量不会被tf.name_scope()当中的名字所影响."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.variable_scope() \n",
    "如果想要达到重复利用变量的效果, 我们就要使用 tf.variable_scope(), 并搭配 tf.get_variable() 这种方式产生和提取变量. 不像 tf.Variable() 每次都会产生新的变量, tf.get_variable() 如果遇到了同样名字的变量时, 它会单纯的提取这个同样名字的变量(避免产生新变量). 而在重复使用的时候, 一定要在代码中强调 scope.reuse_variables(), 否则系统将会报错, 以为你只是单纯的不小心重复使用到了一个变量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_variable_scope/var3:0\n",
      "[ 3.]\n",
      "a_variable_scope/var3:0\n",
      "[ 3.]\n",
      "a_variable_scope/var4:0\n",
      "[ 4.]\n",
      "a_variable_scope/var4_1:0\n",
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"a_variable_scope\") as scope:\n",
    "    initializer = tf.constant_initializer(value=3)\n",
    "    var3 = tf.get_variable(name='var3', shape=[1], dtype=tf.float32, initializer=initializer)\n",
    "    scope.reuse_variables()\n",
    "    var3_reuse = tf.get_variable(name='var3',)\n",
    "    var4 = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)\n",
    "    var4_reuse = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(var3.name)            # a_variable_scope/var3:0\n",
    "    print(sess.run(var3))       # [ 3.]\n",
    "    print(var3_reuse.name)      # a_variable_scope/var3:0\n",
    "    print(sess.run(var3_reuse)) # [ 3.]\n",
    "    print(var4.name)            # a_variable_scope/var4:0\n",
    "    print(sess.run(var4))       # [ 4.]\n",
    "    print(var4_reuse.name)      # a_variable_scope/var4_1:0\n",
    "    print(sess.run(var4_reuse)) # [ 4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN应用例子 \n",
    "RNN 例子的代码在[这里](https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf22_scope/tf22_RNN_scope.py), 整个 RNN 的结构已经在这里定义好了. 在 training RNN 和 test RNN 的时候, RNN 的 time_steps 会有不同的取值, 这将会影响到整个 RNN 的结构, 所以导致在 test 的时候, 不能单纯地使用 training 时建立的那个 RNN. 但是 training RNN 和 test RNN 又必须是有同样的 weights biases 的参数. 所以, 这时, 就是使用 reuse variable 的好时机.\n",
    "\n",
    "首先定义training 和 test 的不同参数.\n",
    "\n",
    "    class TrainConfig:\n",
    "        batch_size = 20\n",
    "        time_steps = 20\n",
    "        input_size = 10\n",
    "        output_size = 2\n",
    "        cell_size = 11\n",
    "        learning_rate = 0.01\n",
    "\n",
    "\n",
    "    class TestConfig(TrainConfig):\n",
    "        time_steps = 1\n",
    "\n",
    "    train_config = TrainConfig()\n",
    "    test_config = TestConfig()\n",
    "然后让 train_rnn 和 test_rnn 在同一个 tf.variable_scope('rnn') 之下. 并且定义 scope.reuse_variables(), 使我们能把 train_rnn 的所有 weights, biases 参数全部绑定到 test_rnn 中. 这样, 不管两者的 time_steps 有多不同, 结构有多不同, train_rnn W, b 参数更新成什么样, test_rnn 的参数也更新成什么样.\n",
    "\n",
    "    with tf.variable_scope('rnn') as scope:\n",
    "        sess = tf.Session()\n",
    "        train_rnn = RNN(train_config)\n",
    "        scope.reuse_variables()\n",
    "        test_rnn = RNN(test_config)\n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "Batch normalization 是一种解决深度神经网络层数太多, 而没办法有效前向传递(forward propagate)的问题. 因为每一层的输出值都会有不同的 均值(mean) 和 方差(deviation), 所以输出数据的分布也不一样, 如下图, 从左到右是每一层的输入数据分布, 上排的没有 Batch normalization, 下排的有 Batch normalization.\n",
    "![image](https://morvanzhou.github.io/static/results/tensorflow/5_13_01.png)\n",
    "我们以前说过, 为了更有效的学习数据, 我们会对数据预处理, 进行 normalization (请参考我制作的 为什么要特征标准化). 而现在请想象, 我们可以把 “每层输出的值” 都看成 “后面一层所接收的数据”. 对每层都进行一次 normalization 会不会更好呢? 这就是 Batch normalization 方法的由来."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ACTIVATION = tf.nn.relu # 每一层都使用 relu \n",
    "N_LAYERS = 7            # 一共7层隐藏层\n",
    "N_HIDDEN_UNITS = 30     # 每个层隐藏层有 30 个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed=1):\n",
    "    # reproducible\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_his(inputs,inputs_norm):\n",
    "    # plot histogram for the inputs of every layer\n",
    "#     enumerate(iterable[, start]) -> iterator for index, value of iterable\n",
    "    for j,all_inputs in  enumerate([inputs,inputs_norm]):\n",
    "        for i,input in enumerate(all_inputs):\n",
    "            plt.subplot(2, len(all_inputs), j*len(all_inputs)+(i+1))\n",
    "            plt.cla() #clear current axes\n",
    "            if i==0:\n",
    "                the_range = (-7,10)\n",
    "            else:\n",
    "                the_range = (-1,1)\n",
    "            plt.hist(input.ravel(),bins=15,range=the_range,color='#FF5733') #bins 柱状图条数\n",
    "            plt.yticks(())\n",
    "            if j == 1:\n",
    "                plt.xticks(the_range)\n",
    "            else:\n",
    "                plt.xticks(())\n",
    "            ax = plt.gca()\n",
    "            ax.spines['right'].set_color('none')\n",
    "            ax.spines['top'].set_color('none')\n",
    "        plt.title(\"%s normalizing\" % (\"Without\" if j == 0 else \"With\"))\n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(xs,ys,norm):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
